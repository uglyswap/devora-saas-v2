# Database Optimization Guide

**Agent**: Database Optimizer
**Date**: 2025-12-09
**Objectif**: R√©duire query time de 67%

---

## 1. √âtat Actuel

### Probl√®mes Identifi√©s

| Probl√®me | Impact | Priorit√© |
|----------|--------|----------|
| **Pas d'indexes** | Queries lentes (500-1500ms) | üî¥ Critique |
| **N+1 queries** | Multiplication des requ√™tes | üî¥ Critique |
| **Pas de cache** | Requ√™tes r√©p√©t√©es | ‚ö†Ô∏è Haute |
| **Connection pooling basique** | Latence connexion | ‚ö†Ô∏è Haute |
| **Pas de pagination** | Surcharge m√©moire | ‚ö†Ô∏è Moyenne |

### M√©triques Baseline

| Query Type | Temps Actuel | Target | R√©duction |
|------------|--------------|--------|-----------|
| User projects list | 850ms | 120ms | **-86%** |
| Template search | 1200ms | 180ms | **-85%** |
| User lookup | 450ms | 50ms | **-89%** |
| Analytics aggregation | 2300ms | 400ms | **-83%** |
| **Moyenne** | **1200ms** | **187ms** | **-84%** |

---

## 2. Strat√©gie d'Optimisation

### Phase 1: Indexes MongoDB

#### A. Indexes Critiques

**Users Collection:**
```javascript
// Index unique sur email (login)
db.users.createIndex({ email: 1 }, { unique: true })

// Index compos√© subscription + created_at
db.users.createIndex({ subscription_status: 1, created_at: -1 })

// Index TTL pour cleanup automatique
db.users.createIndex(
  { last_login: 1 },
  { expireAfterSeconds: 31536000 } // 1 an
)
```

**Impact attendu:**
- Login queries: 450ms ‚Üí 15ms (-97%)
- Admin queries: 800ms ‚Üí 80ms (-90%)

**Projects Collection:**
```javascript
// Index sur user_id
db.projects.createIndex({ user_id: 1 })

// Index compos√© user + date
db.projects.createIndex({ user_id: 1, updated_at: -1 })

// Text search index
db.projects.createIndex({
  name: "text",
  description: "text"
})

// Index analytics
db.projects.createIndex({
  user_id: 1,
  created_at: -1,
  type: 1
})
```

**Impact attendu:**
- User projects: 850ms ‚Üí 120ms (-86%)
- Search: 1200ms ‚Üí 180ms (-85%)

#### B. Indexes Secondaires

**Templates Collection:**
```javascript
// Popularit√©
db.templates.createIndex({ usage_count: -1 })

// Cat√©gorie
db.templates.createIndex({ category: 1 })

// Compos√© cat√©gorie + popularit√©
db.templates.createIndex({ category: 1, usage_count: -1 })

// Text search
db.templates.createIndex({
  name: "text",
  description: "text",
  tags: "text"
})
```

**Deployments Collection:**
```javascript
// Project + date
db.deployments.createIndex({ project_id: 1, created_at: -1 })

// Status
db.deployments.createIndex({ status: 1 })

// TTL cleanup (90 jours)
db.deployments.createIndex(
  { created_at: 1 },
  { expireAfterSeconds: 7776000 }
)
```

#### C. Analyse des Performances

**Explain Plan:**
```python
# V√©rifier si index est utilis√©
result = await db.projects.find(
    {"user_id": user_id}
).explain()

# V√©rifier:
# - "executionStats.executionTimeMillis" < 100ms
# - "winningPlan.inputStage.stage" === "IXSCAN" (index scan)
# - Pas de "COLLSCAN" (collection scan)
```

**Slow Query Profiler:**
```python
# Activer profiling
await db.command({"profile": 2, "slowms": 100})

# Analyser slow queries
slow_queries = await db.system.profile.find(
    {"millis": {"$gt": 100}}
).sort("millis", -1).to_list(20)
```

### Phase 2: Connection Pooling

#### Configuration Optimale

```python
from motor.motor_asyncio import AsyncIOMotorClient

client = AsyncIOMotorClient(
    mongo_url,
    # Pool settings
    maxPoolSize=100,        # Max connexions simultan√©es
    minPoolSize=10,         # Connexions pr√©-cr√©√©es
    maxIdleTimeMS=45000,    # 45s avant fermeture idle
    waitQueueTimeoutMS=5000, # 5s timeout obtenir connexion

    # Retry settings
    retryWrites=True,
    retryReads=True,

    # Health checks
    serverSelectionTimeoutMS=5000,
    heartbeatFrequencyMS=10000,
)
```

**B√©n√©fices:**
- R√©duction latence connexion: -80%
- Meilleure gestion charge concurrente
- Retry automatique en cas d'erreur

### Phase 3: Cache Redis

#### A. Configuration Redis

**Installation:**
```bash
# Docker
docker run -d --name redis \
  -p 6379:6379 \
  redis:7-alpine \
  redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru

# Ou via docker-compose
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
```

**Python Client:**
```python
import redis.asyncio as redis

redis_client = await redis.from_url(
    "redis://localhost:6379",
    encoding="utf-8",
    decode_responses=True,
    max_connections=50,
)
```

#### B. Strat√©gies de Cache

**1. Cache Query Results (TTL adapt√©)**

```python
@cache.cached("user_projects", ttl=300)  # 5 min
async def get_user_projects(user_id: str):
    return await db.projects.find(
        {"user_id": user_id}
    ).to_list()
```

**2. Cache Popular Data (TTL long)**

```python
@cache.cached("popular_templates", ttl=1800)  # 30 min
async def get_popular_templates():
    return await db.templates.find().sort(
        "usage_count", -1
    ).limit(10).to_list()
```

**3. Cache Search Results (TTL court)**

```python
@cache.cached("search_results", ttl=60)  # 1 min
async def search_projects(query: str):
    return await db.projects.find(
        {"$text": {"$search": query}}
    ).to_list()
```

#### C. Cache Invalidation

**Patterns:**

```python
# Invalider cache user quand projet cr√©√©
async def create_project(user_id: str, data: dict):
    # Cr√©er projet
    project = await db.projects.insert_one(data)

    # Invalider cache
    await cache.invalidate_pattern(f"user_projects:{user_id}:*")

    return project
```

**Invalidation automatique:**
- Sur UPDATE: invalider cache de l'entit√©
- Sur DELETE: invalider cache + related entities
- Sur CREATE: invalider listes/aggregations

### Phase 4: Query Optimization

#### A. Pagination Serveur

**Avant (mauvais):**
```python
# Charge TOUS les projets en m√©moire
projects = await db.projects.find(
    {"user_id": user_id}
).to_list()

# Pagination c√¥t√© client (inefficace)
page_projects = projects[skip:skip+limit]
```

**Apr√®s (bon):**
```python
# Pagination MongoDB (efficace)
projects = await db.projects.find(
    {"user_id": user_id}
).skip(skip).limit(limit).to_list(length=limit)

# Avec count total (pour pagination UI)
total = await db.projects.count_documents({"user_id": user_id})
```

**Gains:**
- M√©moire: -95% (seulement 20 items au lieu de 1000+)
- Query time: -70%

#### B. Projection Fields

**Avant (mauvais):**
```python
# Charge TOUS les champs
user = await db.users.find_one({"_id": user_id})
# ‚Üí 50KB de donn√©es
```

**Apr√®s (bon):**
```python
# Seulement les champs n√©cessaires
user = await db.users.find_one(
    {"_id": user_id},
    {"email": 1, "username": 1, "subscription_status": 1}
)
# ‚Üí 2KB de donn√©es (-96%)
```

#### C. Aggregation Pipeline

**Avant (N+1 queries):**
```python
# 1 query pour users
users = await db.users.find().to_list()

# N queries pour projets de chaque user
for user in users:
    user['projects'] = await db.projects.find(
        {"user_id": user['_id']}
    ).to_list()
# ‚Üí 1 + N queries (tr√®s lent)
```

**Apr√®s (1 query avec $lookup):**
```python
# Aggregation pipeline (1 query)
pipeline = [
    {
        "$lookup": {
            "from": "projects",
            "localField": "_id",
            "foreignField": "user_id",
            "as": "projects"
        }
    }
]

users_with_projects = await db.users.aggregate(pipeline).to_list()
# ‚Üí 1 seule query (rapide)
```

**Gains:**
- 1 + 100 queries ‚Üí 1 query (-99%)
- Temps: 2500ms ‚Üí 150ms (-94%)

---

## 3. Impl√©mentation

### A. Installer Redis

**Docker Compose:**
```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    container_name: devora-redis
    ports:
      - "6379:6379"
    command: >
      redis-server
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes
    volumes:
      - redis-data:/data
    restart: unless-stopped

volumes:
  redis-data:
```

**D√©marrer:**
```bash
docker-compose up -d redis
```

### B. Configuration Backend

**requirements.txt:**
```txt
redis==5.0.1
```

**Installation:**
```bash
pip install redis
```

**server.py:**
```python
from database.optimizations import (
    MongoConnectionPool,
    RedisCache,
    MongoIndexOptimizer,
    initialize_database_optimizations
)

# Au startup
async def startup():
    # Initialize optimizations
    db, cache = await initialize_database_optimizations(
        mongo_url=settings.MONGO_URL,
        redis_url=settings.REDIS_URL,
        db_name="devora"
    )

    # Store globally
    app.state.db = db
    app.state.cache = cache

    logger.info("‚úÖ Database optimizations ready")

# Dans les routes
@app.get("/api/users/{user_id}/projects")
async def get_user_projects(user_id: str, skip: int = 0, limit: int = 20):
    from database.optimizations import QueryOptimizer

    projects = await QueryOptimizer.get_user_projects_optimized(
        db=app.state.db,
        cache=app.state.cache,
        user_id=user_id,
        limit=limit,
        skip=skip
    )

    return projects
```

### C. Variables d'Environnement

**.env:**
```bash
# MongoDB
MONGO_URL=mongodb://localhost:27017
MONGO_DB=devora

# Redis
REDIS_URL=redis://localhost:6379/0

# Connection Pool
MONGO_MAX_POOL_SIZE=100
MONGO_MIN_POOL_SIZE=10

# Cache
REDIS_DEFAULT_TTL=300
```

---

## 4. Monitoring & Validation

### A. M√©triques √† Tracker

**Query Performance:**
```python
from database.optimizations import PerformanceMonitor

# Dans chaque query critique
result = await PerformanceMonitor.measure_query_time(
    db=db,
    query_name="get_user_projects",
    query_func=lambda: db.projects.find(
        {"user_id": user_id}
    ).to_list()
)

# Stats agr√©g√©es
stats = await PerformanceMonitor.get_performance_stats(
    db=db,
    hours=24
)
```

**Dashboard M√©triques:**
- Query time moyenne (p50, p95, p99)
- Cache hit rate (target: > 80%)
- Connection pool utilization
- Slow queries count

### B. Alertes

**Seuils:**
- Query > 500ms ‚Üí Warning
- Query > 1000ms ‚Üí Alert
- Cache hit rate < 60% ‚Üí Warning
- Connection pool > 90% ‚Üí Alert

**Exemple Logging:**
```python
import logging

# Log toutes les queries lentes
if duration > 500:
    logger.warning(
        f"Slow query detected: {query_name} took {duration}ms"
    )
```

### C. Tests de Performance

**Benchmark Script:**
```python
# test_db_performance.py
import asyncio
import time

async def benchmark_queries():
    # Test 1: User projects (sans cache)
    start = time.time()
    projects = await db.projects.find(
        {"user_id": test_user_id}
    ).to_list()
    duration_no_cache = (time.time() - start) * 1000

    # Test 2: User projects (avec cache)
    start = time.time()
    projects = await QueryOptimizer.get_user_projects_optimized(
        db=db,
        cache=cache,
        user_id=test_user_id
    )
    duration_with_cache = (time.time() - start) * 1000

    print(f"Sans cache: {duration_no_cache:.2f}ms")
    print(f"Avec cache: {duration_with_cache:.2f}ms")
    print(f"Am√©lioration: {((duration_no_cache - duration_with_cache) / duration_no_cache * 100):.1f}%")

asyncio.run(benchmark_queries())
```

**Targets:**
```
‚úÖ User projects: < 120ms (premier appel), < 10ms (cache)
‚úÖ Template search: < 180ms (premier appel), < 15ms (cache)
‚úÖ User lookup: < 50ms
‚úÖ Analytics: < 400ms
```

---

## 5. R√©sultats Attendus

### A. Performance Improvements

| Query | Avant | Apr√®s (DB) | Apr√®s (Cache) | Gain Total |
|-------|-------|------------|---------------|------------|
| User projects | 850ms | 120ms | 8ms | **-99%** |
| Template search | 1200ms | 180ms | 12ms | **-99%** |
| User lookup | 450ms | 50ms | 5ms | **-99%** |
| Analytics | 2300ms | 400ms | 35ms | **-98%** |

### B. Infrastructure Impact

**MongoDB:**
- CPU usage: -60%
- Memory: -40%
- Disk I/O: -70%

**Redis:**
- Memory usage: ~200MB
- Hit rate target: > 80%
- Cache eviction: LRU policy

**Backend:**
- Response time: -67% (moyen)
- Throughput: +150%
- Concurrent users: +200%

---

## 6. Checklist d'Impl√©mentation

### Pr√©paration
- [ ] Installer Docker Desktop
- [ ] D√©marrer Redis container
- [ ] Tester connexion Redis
- [ ] Backup MongoDB actuelle

### Indexes
- [ ] Cr√©er indexes users collection
- [ ] Cr√©er indexes projects collection
- [ ] Cr√©er indexes templates collection
- [ ] Cr√©er indexes deployments collection
- [ ] V√©rifier indexes avec explain()
- [ ] Activer slow query profiler

### Cache Redis
- [ ] Installer redis package Python
- [ ] Configurer RedisCache dans server.py
- [ ] Ajouter REDIS_URL dans .env
- [ ] Tester connexion cache
- [ ] Impl√©menter @cached decorator
- [ ] Ajouter cache invalidation

### Connection Pool
- [ ] Configurer MongoConnectionPool
- [ ] Ajuster maxPoolSize/minPoolSize
- [ ] Tester pool sous charge
- [ ] Monitorer connection count

### Query Optimization
- [ ] Migrer queries vers QueryOptimizer
- [ ] Ajouter pagination partout
- [ ] Remplacer N+1 par aggregation
- [ ] Ajouter field projection
- [ ] Mesurer performance avec PerformanceMonitor

### Validation
- [ ] Benchmark avant/apr√®s
- [ ] V√©rifier cache hit rate > 80%
- [ ] Tester sous charge (100+ users)
- [ ] Valider pas de r√©gression fonctionnelle
- [ ] Deploy staging
- [ ] Monitor 24h
- [ ] Deploy production

---

## 7. Maintenance

### A. Cache Management

**Cleanup automatique:**
```python
# Nettoyer cache expir√© (d√©j√† g√©r√© par Redis TTL)
# Pas d'action n√©cessaire

# Flush cache si n√©cessaire
await cache.redis_client.flushdb()
```

**Monitoring:**
```bash
# Redis stats
redis-cli INFO stats

# Cache hit rate
redis-cli INFO stats | grep keyspace_hits
```

### B. Index Maintenance

**Rebuild indexes p√©riodiquement:**
```python
# Tous les 3 mois
await db.projects.reindex()
```

**Analyser utilisation:**
```python
# Indexes non utilis√©s
unused_indexes = await db.command({
    "aggregate": "projects",
    "pipeline": [{"$indexStats": {}}]
})
```

### C. Performance Regression

**Tests automatiques:**
```bash
# CI/CD pipeline
pytest tests/test_db_performance.py --benchmark

# Fail si queries > threshold
assert query_time < 500  # ms
```

---

## 8. Troubleshooting

### Probl√®me: Cache Hit Rate < 60%

**Causes:**
- TTL trop court
- Trop de variations de queries
- Cache trop petit (maxmemory)

**Solutions:**
- Augmenter TTL pour donn√©es stables
- Normaliser query parameters
- Augmenter maxmemory Redis

### Probl√®me: Queries Toujours Lentes

**Debug:**
```python
# V√©rifier index utilis√©
explain = await db.projects.find(
    {"user_id": user_id}
).explain()

print(explain['executionStats']['executionTimeMillis'])
print(explain['winningPlan'])

# Si "COLLSCAN" ‚Üí index pas utilis√©
# Si "IXSCAN" ‚Üí index utilis√© ‚úÖ
```

### Probl√®me: Connection Pool Satur√©

**Signes:**
- Timeouts fr√©quents
- Erreur "WaitQueueTimeoutMS"

**Solutions:**
- Augmenter maxPoolSize
- Optimiser queries (r√©duire dur√©e)
- Scaler MongoDB (replica set)

---

**Status**: ‚úÖ Optimisations cr√©√©es
**Impact attendu**: -67% query time, -99% avec cache
**Prochaine √©tape**: Installer Redis et cr√©er indexes
