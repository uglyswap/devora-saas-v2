"""
Script de test pour v√©rifier les utilitaires Devora.

Usage:
    python orchestration/test_utils.py
"""

import asyncio
import sys
import io
from pathlib import Path

# Fix encoding pour Windows
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))


async def test_llm_client():
    """Test du LLMClient."""
    print("\n=== Test LLMClient ===")

    from orchestration.utils import create_llm_client, ModelType

    try:
        async with await create_llm_client() as client:
            print("‚úì Client LLM cr√©√©")

            # Test de comptage de tokens
            text = "Ceci est un test simple."
            tokens = client.count_tokens(text)
            print(f"‚úì Comptage tokens: {tokens} tokens pour '{text}'")

            # Test de compl√©tion (n√©cessite OPENROUTER_API_KEY)
            try:
                response = await client.complete(
                    messages=[{"role": "user", "content": "Dis bonjour en 5 mots"}],
                    model=ModelType.HAIKU,  # Plus rapide
                    max_tokens=50,
                )
                print(f"‚úì Compl√©tion r√©ussie: {response.content[:50]}...")
                print(f"  Tokens utilis√©s: {response.tokens_used}")
                print(f"  Mod√®le: {response.model}")
            except Exception as e:
                print(f"‚ö† Compl√©tion ignor√©e (cl√© API requise): {e}")

    except Exception as e:
        print(f"‚úó Erreur LLMClient: {e}")
        return False

    return True


def test_logger():
    """Test du Logger."""
    print("\n=== Test Logger ===")

    from orchestration.utils import get_logger

    try:
        logger = get_logger(__name__)
        print("‚úì Logger cr√©√©")

        # Test logging simple
        logger.debug("Message de debug")
        logger.info("Message d'information")
        logger.warning("Message d'avertissement")
        print("‚úì Logging simple OK")

        # Test logging structur√©
        logger.info("√âv√©nement", user_id=123, action="test", status="success")
        print("‚úì Logging structur√© OK")

        # Test contexte
        context_logger = logger.with_context(module="test", session="test_123")
        context_logger.info("Message avec contexte")
        print("‚úì Logging avec contexte OK")

    except Exception as e:
        print(f"‚úó Erreur Logger: {e}")
        return False

    return True


def test_token_manager():
    """Test du TokenManager."""
    print("\n=== Test TokenManager ===")

    from orchestration.utils import TokenManager, count_tokens

    try:
        tm = TokenManager()
        print("‚úì TokenManager cr√©√©")

        # Test comptage
        text = "Ceci est un exemple de texte √† analyser pour compter les tokens."
        tokens = tm.count_tokens(text, model="claude")
        print(f"‚úì Comptage: {tokens} tokens pour {len(text)} caract√®res")

        # Test avec fonction utilitaire
        tokens2 = count_tokens(text)
        assert tokens == tokens2, "Les deux m√©thodes doivent donner le m√™me r√©sultat"
        print("‚úì Fonction utilitaire OK")

        # Test messages
        messages = [
            {"role": "system", "content": "Tu es un assistant."},
            {"role": "user", "content": "Bonjour!"},
        ]
        msg_tokens = tm.count_messages_tokens(messages)
        print(f"‚úì Messages: {msg_tokens} tokens pour {len(messages)} messages")

        # Test limites
        limit = tm.get_model_limit("anthropic/claude-3.5-sonnet")
        print(f"‚úì Limite Claude Sonnet: {limit:,} tokens")

        # Test v√©rification de capacit√©
        fits, used, available = tm.check_context_fit(
            messages,
            model="anthropic/claude-3.5-sonnet",
            max_completion_tokens=4096,
        )
        print(f"‚úì V√©rification: {'OK' if fits else 'Trop grand'} ({used} utilis√©s, {available} disponibles)")

        # Test compression
        long_text = "test " * 1000
        result = tm.compress_context(
            text=long_text,
            target_tokens=100,
            strategy="truncate",
        )
        print(f"‚úì Compression: {result.original_tokens} ‚Üí {result.compressed_tokens} tokens (ratio: {result.compression_ratio:.2%})")

    except Exception as e:
        print(f"‚úó Erreur TokenManager: {e}")
        return False

    return True


async def test_progress_emitter():
    """Test du ProgressEmitter."""
    print("\n=== Test ProgressEmitter ===")

    from orchestration.utils import ProgressEmitter, EventType

    try:
        emitter = ProgressEmitter(session_id="test_session")
        print(f"‚úì ProgressEmitter cr√©√© (session: {emitter.session_id})")

        # Test callback
        events_received = []

        def callback(event):
            events_received.append(event.type.value)

        emitter.on_any(callback)
        print("‚úì Callback enregistr√©")

        # Test √©mission
        await emitter.workflow_start("test_workflow", {"type": "test"})
        await emitter.agent_start("test_agent", "Test Agent")
        await emitter.task_progress("task_1", 0.5, "En cours...")
        await emitter.agent_complete("test_agent", {"status": "success"})
        await emitter.workflow_complete("test_workflow", {"result": "ok"})

        print(f"‚úì {len(events_received)} √©v√©nements √©mis")

        # Test r√©cup√©ration
        all_events = emitter.get_events(limit=10)
        print(f"‚úì {len(all_events)} √©v√©nements dans le buffer")

        # Test statistiques
        stats = emitter.get_stats()
        print(f"‚úì Stats: {stats['total_events']} √©v√©nements totaux")

        # Test SSE
        queue = await emitter.create_sse_stream()
        print("‚úì Queue SSE cr√©√©e")

        # √âmettre un √©v√©nement de test
        await emitter.log("info", "Test SSE")

        # V√©rifier que l'√©v√©nement est dans la queue
        event = await asyncio.wait_for(queue.get(), timeout=1.0)
        sse_format = event.to_sse()
        print(f"‚úì Format SSE OK: {sse_format[:50]}...")

        emitter.remove_sse_stream(queue)

    except Exception as e:
        print(f"‚úó Erreur ProgressEmitter: {e}")
        return False

    return True


def test_prompts():
    """Test des templates de prompts."""
    print("\n=== Test Templates Prompts ===")

    from orchestration.templates import (
        RouterPrompts,
        PlannerPrompts,
        format_prompt,
        create_messages,
    )

    try:
        # Test format_prompt
        prompt = format_prompt(
            RouterPrompts.ANALYZE_REQUEST,
            query="Test query",
            context="Test context",
        )
        assert "Test query" in prompt
        assert "Test context" in prompt
        print("‚úì format_prompt OK")

        # Test create_messages
        messages = create_messages(
            system_prompt=RouterPrompts.SYSTEM,
            user_prompt=prompt,
        )
        assert len(messages) == 2
        assert messages[0]["role"] == "system"
        assert messages[1]["role"] == "user"
        print("‚úì create_messages OK")

        # Test avec historique
        messages_with_history = create_messages(
            system_prompt=PlannerPrompts.SYSTEM,
            user_prompt="New prompt",
            history=[
                {"role": "user", "content": "Previous question"},
                {"role": "assistant", "content": "Previous answer"},
            ],
        )
        assert len(messages_with_history) == 4
        print("‚úì create_messages avec historique OK")

        # V√©rifier que tous les prompts sont accessibles
        assert hasattr(RouterPrompts, "SYSTEM")
        assert hasattr(PlannerPrompts, "CREATE_PLAN")
        print("‚úì Tous les prompts accessibles")

    except Exception as e:
        print(f"‚úó Erreur Templates Prompts: {e}")
        return False

    return True


def test_responses():
    """Test des templates de r√©ponses."""
    print("\n=== Test Templates R√©ponses ===")

    from orchestration.templates import (
        Metadata,
        ResponseStatus,
        RouterResponse,
        Task,
        TaskType,
        create_success_response,
        create_error_response,
    )

    try:
        # Test Metadata
        metadata = Metadata(
            agent_id="test_agent",
            agent_type="test",
            execution_time=1.23,
            tokens_used=456,
        )
        print("‚úì Metadata cr√©√©")

        # Test RouterResponse
        response = RouterResponse(
            status=ResponseStatus.SUCCESS,
            metadata=metadata,
            intent="Test intent",
            complexity="medium",
            workflow="test_workflow",
            required_agents=["agent1", "agent2"],
            estimated_steps=3,
        )
        print("‚úì RouterResponse cr√©√©")

        # Test to_dict
        response_dict = response.to_dict()
        assert "status" in response_dict
        assert "metadata" in response_dict
        assert response_dict["data"]["intent"] == "Test intent"
        print("‚úì to_dict() OK")

        # Test Task
        task = Task(
            id="task_1",
            description="Test task",
            type=TaskType.IMPLEMENTATION,
            priority=1,
        )
        task_dict = task.to_dict()
        assert task_dict["type"] == "implementation"
        print("‚úì Task OK")

        # Test create_success_response
        success = create_success_response(
            agent_id="test",
            agent_type="test",
            data={"result": "ok"},
        )
        assert success.is_success()
        print("‚úì create_success_response OK")

        # Test create_error_response
        error = create_error_response(
            agent_id="test",
            agent_type="test",
            error_message="Test error",
            error_code="ERR_TEST",
        )
        assert not error.is_success()
        assert error.status == ResponseStatus.ERROR
        print("‚úì create_error_response OK")

    except Exception as e:
        import traceback
        print(f"‚úó Erreur Templates R√©ponses: {e}")
        traceback.print_exc()
        return False

    return True


async def main():
    """Ex√©cute tous les tests."""
    print("=" * 60)
    print("TESTS DES UTILITAIRES DEVORA")
    print("=" * 60)

    results = []

    # Tests synchrones
    results.append(("Logger", test_logger()))
    results.append(("TokenManager", test_token_manager()))
    results.append(("Prompts", test_prompts()))
    results.append(("Responses", test_responses()))

    # Tests asynchrones
    results.append(("LLMClient", await test_llm_client()))
    results.append(("ProgressEmitter", await test_progress_emitter()))

    # R√©sum√©
    print("\n" + "=" * 60)
    print("R√âSUM√â DES TESTS")
    print("=" * 60)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "‚úì PASS" if result else "‚úó FAIL"
        print(f"{status} - {name}")

    print(f"\n{passed}/{total} tests r√©ussis")

    if passed == total:
        print("\nüéâ Tous les tests sont pass√©s!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) √©chou√©(s)")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
