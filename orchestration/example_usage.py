"""
Exemple d'utilisation des utilitaires Devora.

Cet exemple montre comment utiliser tous les composants ensemble
pour cr√©er un workflow d'analyse simple.
"""

import asyncio
import json
import os
import sys
import io
from datetime import datetime
from pathlib import Path

# Fix encoding pour Windows
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Configuration de l'environnement (optionnel si .env existe)
# os.environ["OPENROUTER_API_KEY"] = "sk-or-v1-..."

from orchestration.utils import (
    create_llm_client,
    ModelType,
    get_logger,
    TokenManager,
    ProgressEmitter,
    EventType,
)
from orchestration.templates import (
    RouterPrompts,
    PlannerPrompts,
    create_messages,
    format_prompt,
    RouterResponse,
    PlannerResponse,
    Metadata,
    ResponseStatus,
    Task,
    TaskType,
)


async def analyze_request_example():
    """
    Exemple 1: Analyse d'une requ√™te utilisateur avec le Router.
    """
    print("\n" + "=" * 60)
    print("EXEMPLE 1: Analyse de Requ√™te")
    print("=" * 60)

    # Setup des composants
    logger = get_logger(__name__).with_context(example="router_analysis")
    emitter = ProgressEmitter(session_id="example_1")
    tm = TokenManager()

    # Callback pour logger tous les √©v√©nements
    def log_events(event):
        logger.info(f"Event: {event.type.value}", **event.data)

    emitter.on_any(log_events)

    # Requ√™te utilisateur
    user_query = """
    Je veux cr√©er une application web de gestion de t√¢ches collaborative.
    L'application doit permettre:
    - Cr√©er/√©diter/supprimer des t√¢ches
    - Assigner des t√¢ches √† des utilisateurs
    - Suivre la progression en temps r√©el
    - Envoyer des notifications
    - Exporter des rapports
    """

    # D√©marrage du workflow
    await emitter.workflow_start("router_analysis", {"query": user_query[:50] + "..."})

    # Cr√©er le prompt
    prompt = format_prompt(
        RouterPrompts.ANALYZE_REQUEST,
        query=user_query,
        context="Startup tech, √©quipe de 3 devs, budget 2 mois"
    )

    messages = create_messages(
        system_prompt=RouterPrompts.SYSTEM,
        user_prompt=prompt
    )

    # V√©rifier les tokens
    token_count = tm.count_messages_tokens(messages)
    logger.info("Prompt cr√©√©", tokens=token_count)

    print(f"\nüìù Requ√™te utilisateur:")
    print(user_query)
    print(f"\nüìä Tokens du prompt: {token_count}")

    # Simuler l'appel LLM (comment√© car n√©cessite une cl√© API)
    """
    async with await create_llm_client() as llm:
        await emitter.agent_start("router-001", "Router Agent")

        response = await llm.complete(
            messages,
            model=ModelType.SONNET,
            temperature=0.7,
            max_tokens=2048
        )

        logger.info("R√©ponse LLM re√ßue", tokens=response.tokens_used)

        # Parser la r√©ponse JSON
        analysis = json.loads(response.content)

        # Cr√©er la r√©ponse structur√©e
        metadata = Metadata(
            agent_id="router-001",
            agent_type="router",
            execution_time=1.5,
            tokens_used=response.tokens_used,
            model_used=response.model
        )

        result = RouterResponse(
            status=ResponseStatus.SUCCESS,
            metadata=metadata,
            intent=analysis["intent"],
            complexity=analysis["complexity"],
            workflow=analysis["workflow"],
            required_agents=analysis["required_agents"],
            estimated_steps=analysis["estimated_steps"],
            reasoning=analysis["reasoning"]
        )

        print(f"\n‚úÖ Analyse termin√©e:")
        print(f"  - Intention: {result.intent}")
        print(f"  - Complexit√©: {result.complexity}")
        print(f"  - Workflow: {result.workflow}")
        print(f"  - Agents requis: {', '.join(result.required_agents)}")
        print(f"  - √âtapes estim√©es: {result.estimated_steps}")

        await emitter.agent_complete("router-001", result.to_dict())
    """

    # Simulation avec des donn√©es factices
    print("\n‚ö†Ô∏è  Mode simulation (cl√© API non configur√©e)")
    simulation_result = {
        "intent": "D√©veloppement d'application web collaborative",
        "complexity": "high",
        "workflow": "full_stack_development",
        "required_agents": ["business_analyst", "frontend_dev", "backend_dev", "qa_engineer"],
        "estimated_steps": 8,
        "reasoning": "Projet complexe n√©cessitant frontend, backend, temps r√©el et notifications"
    }

    print(f"\n‚úÖ Analyse simul√©e:")
    for key, value in simulation_result.items():
        print(f"  - {key}: {value}")

    await emitter.workflow_complete("router_analysis", simulation_result)

    # Stats finales
    stats = emitter.get_stats()
    print(f"\nüìà Statistiques:")
    print(f"  - Total √©v√©nements: {stats['total_events']}")
    print(f"  - √âv√©nements par type: {stats['events_by_type']}")


async def planning_example():
    """
    Exemple 2: Cr√©ation d'un plan de projet avec le Planner.
    """
    print("\n" + "=" * 60)
    print("EXEMPLE 2: Planification de Projet")
    print("=" * 60)

    logger = get_logger(__name__).with_context(example="planning")
    emitter = ProgressEmitter(session_id="example_2")
    tm = TokenManager()

    # Objectif du projet
    objective = "D√©velopper une API REST pour la gestion de t√¢ches avec authentification JWT"

    # Contraintes
    constraints = """
    - Budget: 2 semaines
    - Stack: FastAPI + PostgreSQL
    - MVP seulement
    - D√©ploiement sur AWS
    """

    # Contexte
    context = """
    - √âquipe de 2 d√©veloppeurs backend
    - Infrastructure AWS existante
    - Base de code existante √† √©tendre
    """

    await emitter.workflow_start("planning", {"objective": objective})

    # Cr√©er le prompt
    prompt = format_prompt(
        PlannerPrompts.CREATE_PLAN,
        objective=objective,
        constraints=constraints,
        context=context
    )

    messages = create_messages(
        system_prompt=PlannerPrompts.SYSTEM,
        user_prompt=prompt
    )

    token_count = tm.count_messages_tokens(messages)
    logger.info("Plan prompt cr√©√©", tokens=token_count)

    print(f"\nüéØ Objectif: {objective}")
    print(f"\nüìä Tokens: {token_count}")

    # Simulation du plan
    print("\n‚ö†Ô∏è  Mode simulation (cl√© API non configur√©e)")

    simulated_plan = {
        "tasks": [
            {
                "id": "task_1",
                "description": "D√©finir les sch√©mas de donn√©es et endpoints API",
                "type": "planning",
                "priority": 1,
                "dependencies": [],
                "estimated_duration": "4 heures",
                "resources_needed": ["OpenAPI spec"],
                "success_criteria": ["Sch√©mas Pydantic d√©finis", "Routes document√©es"]
            },
            {
                "id": "task_2",
                "description": "Impl√©menter l'authentification JWT",
                "type": "implementation",
                "priority": 1,
                "dependencies": ["task_1"],
                "estimated_duration": "8 heures",
                "resources_needed": ["JWT library", "database access"],
                "success_criteria": ["Login/register fonctionnels", "Tests passent"]
            },
            {
                "id": "task_3",
                "description": "Cr√©er les endpoints CRUD pour les t√¢ches",
                "type": "implementation",
                "priority": 2,
                "dependencies": ["task_1", "task_2"],
                "estimated_duration": "12 heures",
                "resources_needed": ["database", "auth middleware"],
                "success_criteria": ["CRUD complet", "Validation des donn√©es", "Tests unitaires"]
            },
            {
                "id": "task_4",
                "description": "Tests d'int√©gration et documentation",
                "type": "validation",
                "priority": 3,
                "dependencies": ["task_3"],
                "estimated_duration": "6 heures",
                "resources_needed": ["Pytest", "OpenAPI generator"],
                "success_criteria": ["Couverture > 80%", "Docs compl√®tes"]
            }
        ],
        "execution_order": ["task_1", "task_2", "task_3", "task_4"],
        "parallel_tasks": [],
        "milestones": [
            {
                "name": "API d√©finie",
                "tasks": ["task_1"],
                "deliverable": "Sp√©cification OpenAPI"
            },
            {
                "name": "Auth impl√©ment√©e",
                "tasks": ["task_2"],
                "deliverable": "Endpoints login/register"
            },
            {
                "name": "MVP ready",
                "tasks": ["task_3", "task_4"],
                "deliverable": "API fonctionnelle test√©e"
            }
        ]
    }

    # Afficher le plan
    print("\n‚úÖ Plan g√©n√©r√©:")
    print(f"\nüìã T√¢ches ({len(simulated_plan['tasks'])}):")
    for task in simulated_plan["tasks"]:
        deps = f" (d√©pend de: {', '.join(task['dependencies'])})" if task['dependencies'] else ""
        print(f"\n  {task['id']} - {task['description']}{deps}")
        print(f"    Type: {task['type']}")
        print(f"    Dur√©e estim√©e: {task['estimated_duration']}")
        print(f"    Crit√®res de succ√®s: {', '.join(task['success_criteria'])}")

    print(f"\nüéØ Milestones ({len(simulated_plan['milestones'])}):")
    for milestone in simulated_plan["milestones"]:
        print(f"  - {milestone['name']}: {milestone['deliverable']}")

    await emitter.workflow_complete("planning", {"tasks_count": len(simulated_plan["tasks"])})


async def streaming_example():
    """
    Exemple 3: Streaming avec progression en temps r√©el.
    """
    print("\n" + "=" * 60)
    print("EXEMPLE 3: Streaming avec Progression")
    print("=" * 60)

    logger = get_logger(__name__).with_context(example="streaming")
    emitter = ProgressEmitter(session_id="example_3")

    await emitter.workflow_start("streaming", {"type": "content_generation"})
    await emitter.agent_start("writer-001", "Writer Agent")

    # Simuler un streaming
    print("\nüìù G√©n√©ration de contenu (simul√©):\n")

    simulated_chunks = [
        "L'orchestration multi-agents est un paradigme puissant ",
        "qui permet de d√©composer des t√¢ches complexes ",
        "en sous-t√¢ches sp√©cialis√©es. ",
        "\n\nChaque agent se concentre sur son domaine d'expertise, ",
        "assurant ainsi une meilleure qualit√© ",
        "et une plus grande efficacit√©. ",
        "\n\nLes agents peuvent travailler en parall√®le ",
        "ou de mani√®re s√©quentielle selon les d√©pendances."
    ]

    full_text = ""
    for i, chunk in enumerate(simulated_chunks):
        # Simuler un d√©lai
        await asyncio.sleep(0.2)

        # Afficher le chunk
        print(chunk, end="", flush=True)
        full_text += chunk

        # √âmettre l'√©v√©nement
        await emitter.llm_stream_chunk(chunk, agent_id="writer-001")

        # Progression
        progress = (i + 1) / len(simulated_chunks)
        await emitter.task_progress(
            "content_generation",
            progress,
            f"{len(full_text)} caract√®res g√©n√©r√©s",
            agent_id="writer-001"
        )

    print("\n\n‚úÖ G√©n√©ration termin√©e!")
    print(f"  - Total: {len(full_text)} caract√®res")
    print(f"  - Chunks: {len(simulated_chunks)}")

    await emitter.agent_complete("writer-001", {"char_count": len(full_text)})
    await emitter.workflow_complete("streaming", {"status": "success"})


async def token_management_example():
    """
    Exemple 4: Gestion avanc√©e des tokens.
    """
    print("\n" + "=" * 60)
    print("EXEMPLE 4: Gestion des Tokens")
    print("=" * 60)

    tm = TokenManager()
    logger = get_logger(__name__).with_context(example="tokens")

    # Cr√©er une longue conversation
    conversation = [
        {"role": "system", "content": "Tu es un assistant expert en d√©veloppement."},
    ]

    # Ajouter beaucoup de messages
    for i in range(50):
        conversation.append({
            "role": "user",
            "content": f"Question {i}: " + "comment faire " * 20
        })
        conversation.append({
            "role": "assistant",
            "content": f"R√©ponse {i}: " + "tu peux utiliser " * 30
        })

    # Compter les tokens
    total_tokens = tm.count_messages_tokens(conversation)
    logger.info("Conversation cr√©√©e", messages=len(conversation), tokens=total_tokens)

    print(f"\nüìä Conversation:")
    print(f"  - Messages: {len(conversation)}")
    print(f"  - Tokens: {total_tokens:,}")

    # V√©rifier si √ßa tient dans le contexte
    model = "anthropic/claude-3.5-sonnet"
    fits, used, available = tm.check_context_fit(
        conversation,
        model=model,
        max_completion_tokens=4096,
        safety_margin=0.1
    )

    print(f"\nüîç V√©rification de capacit√© ({model}):")
    print(f"  - Utilis√©: {used:,} tokens")
    print(f"  - Disponible pour compl√©tion: {available:,} tokens")
    print(f"  - Status: {'‚úÖ OK' if fits else '‚ùå Trop grand'}")

    if not fits:
        # Compression n√©cessaire
        print(f"\n‚ö†Ô∏è  Compression requise!")

        compressed = tm.compress_messages(
            conversation,
            target_tokens=available,
            preserve_system=True,
            preserve_recent=10  # Garder les 10 derniers messages
        )

        compressed_tokens = tm.count_messages_tokens(compressed)

        print(f"\n‚úÖ Compression effectu√©e:")
        print(f"  - Messages originaux: {len(conversation)}")
        print(f"  - Messages compress√©s: {len(compressed)}")
        print(f"  - Ratio: {len(compressed)/len(conversation):.1%}")
        print(f"  - Tokens √©conomis√©s: {total_tokens - compressed_tokens:,}")


async def main():
    """Ex√©cute tous les exemples."""
    print("\n" + "=" * 60)
    print("EXEMPLES D'UTILISATION - UTILITAIRES DEVORA")
    print("=" * 60)
    print(f"\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # V√©rifier la cl√© API
    if not os.getenv("OPENROUTER_API_KEY"):
        print("\n‚ö†Ô∏è  OPENROUTER_API_KEY non configur√©e")
        print("Les exemples s'ex√©cuteront en mode simulation.\n")

    try:
        await analyze_request_example()
        await planning_example()
        await streaming_example()
        await token_management_example()

        print("\n" + "=" * 60)
        print("‚úÖ Tous les exemples termin√©s avec succ√®s!")
        print("=" * 60)

    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
